"""Configure and setup a program execution of the MorphFITS program.

There are two primary settings objects.
1. RuntimeSettings
    Settings related to runtime configurations, such as paths to data
    directories, which stages to run, which products to remake, etc.
2. ConfigSettings
    Settings related to scientific configurations, such as sigma generation
    algorithm.
"""

# Imports


import logging
import shutil
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Annotated, Union, Optional

from tqdm import tqdm
import yaml
from pydantic import BaseModel, StringConstraints

from . import DATA_ROOT
from .utils import logs, misc, science


# Constants


pre_logger = logging.getLogger("CONFIG")
logger = logging.getLogger("CONFIG")
"""Logger objects for this module.
"""


## Filesystem


PATH_STANDARDS_PATH = DATA_ROOT / "paths.yaml"
PATH_STANDARDS_DICT = yaml.safe_load(open(PATH_STANDARDS_PATH, mode="r"))
"""Path and dict representing the MorphFITS filesystem structuring standards.
"""


def index_paths(node: dict[str, str | dict], base: str) -> dict[str, str]:
    """Recursively read the MorphFITS filesystem standards YAML file and
    translate it into a dictionary mapping the path names to their corresponding
    paths, as string templates.

    Each value in the dictionary is a full path template, i.e. 'output_ficlo'
    has the value '[root]/[o]/{F}/{I}/{C}/{L}/{O}' and not '{O}'.

    Parameters
    ----------
    node : dict[str, str | dict]
        Dict mapping str path names to their paths (if a file) or subitems (if a
        directory).
    base : str
        Path prior to this node, as a '/'-delimited string.

    Returns
    -------
    dict[str, str]
        Dict mapping path names to their paths, as string templates.
    """
    # Start with an empty dict for this directory
    index = {}

    # Iterate over each subitem of this directory
    for key, value in node.items():
        # Skip meta keys as they are not subitems
        if key == "_name":
            continue

        # Add files to dictionary
        if isinstance(value, str):
            index[key] = base + "/" + value

        # Add subdirectories to dictionary
        else:
            index[key] = value["_name"]
            if len(base) > 0:
                index[key] = base + "/" + index[key]

            # Concatenate this directory's dictionary with the dictionary
            # generated by the subdirectory
            index = index | index_paths(node=value, base=index[key])

    # Return dictionary from path name to path for this directory
    return index


FILESYSTEM = index_paths(node=PATH_STANDARDS_DICT, base="")
"""Paths to directories and files, as structured and named by `PATH_STANDARDS`.
"""


DEFAULT_INPUT_DIRECTORY_NAME = "input"
DEFAULT_OUTPUT_DIRECTORY_NAME = "output"
DEFAULT_PRODUCT_DIRECTORY_NAME = "products"
DEFAULT_RUN_DIRECTORY_NAME = "runs"
"""Default names for each top-level root directory.
"""


## Required Files


REQUIRED_INPUT_DIRECTORIES = ["input_psfs", "input_fil"]
REQUIRED_OUTPUT_DIRECTORIES = ["output_ficlo", "product_ficlo"]
"""Path names of required directories for MorphFITS to run on a FICLO.
"""


REQUIRED_FIC_INPUTS = ["input_segmap", "input_catalog"]
REQUIRED_FICL_INPUTS = ["input_psf", "exposure", "science", "weights"]
"""Path names of required files for a FICLO to run.
"""


REQUIRED_PRODUCT_FILES = ["stamp"]
REQUIRED_GALFIT_OUTPUT_FILES = ["model_galfit"]
REQUIRED_IMCASCADE_OUTPUT_FILES = ["model_imcascade"]
REQUIRED_PYSERSIC_OUTPUT_FILES = ["model_pysersic"]
"""Path names of required files for a FICLO to be considered successful.
"""


## FICLO


DEFAULT_CATALOG_VERSION = "dja-v7.2"
"""Default catalog version, v7.2 of the DJA catalogue.
"""


# Classes


## Models


class FICL(BaseModel):
    """Configuration model for a single FICL.

    FICL is an abbreviation for the field, image version, catalog version, and
    filter of a JWST science observation. Each FICL corresponds to a single
    observation.

    Attributes
    ----------
    field : str
        Field of observation, e.g. "abell2744clu".
    image_version : str
        Version string of JWST image processing, e.g. "grizli-v7.2".
    catalog_version : str
        Version string of JWST cataloging, e.g. "dja-v7.2".
    filter : str
        Observational filter band, e.g. "f140w".
    objects : list[int]
        Integer IDs of galaxies or cluster targets in catalog.
    pixscale : tuple[float, float]
        Pixel scale along x and y axes, in arcseconds per pixel.

    Notes
    -----
    All strings are converted to lowercase upon validation.
    """

    field: Annotated[str, StringConstraints(to_lower=True)]
    image_version: Annotated[str, StringConstraints(to_lower=True)]
    catalog_version: Annotated[str, StringConstraints(to_lower=True)]
    filter: Annotated[str, StringConstraints(to_lower=True)]
    objects: list[int]
    pixscale: tuple[float, float]

    def __str__(self) -> str:
        return "_".join(
            [self.field, self.image_version, self.catalog_version, self.filter]
        )


## Settings


class StageSettings(BaseModel):
    unzip: bool = True
    product: bool = True
    morphology: bool = True
    catalog: bool = True
    histogram: bool = True
    plot: bool = True
    cleanup: bool = True


class ProductSettings(BaseModel):
    stamps: bool = False
    sigmas: bool = False
    psfs: bool = False
    masks: bool = False
    others: bool = False


class GALFITSettings(BaseModel):
    binary: Path

    def _name():
        return "galfit"


class ImcascadeSettings(BaseModel):
    def _name():
        return "imcascade"


class PysersicSettings(BaseModel):
    def _name():
        return "pysersic"


class PathSettings(BaseModel):
    root: Path
    input: Path
    output: Path
    product: Path
    run: Path


class RuntimeSettings(BaseModel):
    roots: PathSettings
    date_time: datetime
    run_number: int = 1
    process_count: int = 1
    process_id: int = 0
    ficls: list[FICL]
    progress_bar: bool = False
    log_level: str = logging._levelToName[logging.DEBUG]
    stages: Optional[StageSettings] = None
    remake: Optional[ProductSettings] = None
    morphology: Optional[Union[GALFITSettings, ImcascadeSettings, PysersicSettings]] = (
        None
    )

    def setup_directories(self, initialized: bool = True):
        pre_logger.info("Making missing directories.")

        # Create run directory
        get_path(name="run", runtime_settings=self, field=self.ficls[0].field).mkdir(
            parents=True, exist_ok=True
        )

        # If running morphology, only create output and product directories
        if initialized:
            # Iterate over each FICL
            for ficl in self.ficls:
                # Iterate over each object in FICL
                for object in tqdm(ficl.objects, unit="dir", leave=False):
                    # Make leaf FICLO directories
                    for required_directory_name in REQUIRED_OUTPUT_DIRECTORIES:
                        get_path(
                            name=required_directory_name,
                            path_settings=self.roots,
                            ficl=ficl,
                            object=object,
                        ).mkdir(parents=True, exist_ok=True)

        # If running initialize command, only create input directories
        else:
            # Iterate over each FICL
            for ficl in self.ficls:
                # Make PSF and FIL directories, and any parents
                for required_directory_name in REQUIRED_INPUT_DIRECTORIES:
                    get_path(
                        name=required_directory_name,
                        path_settings=self.roots,
                        ficl=ficl,
                    ).mkdir(parents=True, exist_ok=True)

    def setup_loggers(self):
        # Create logger object
        logs.create_logger(
            filename=get_path(
                name="run_log",
                runtime_settings=self,
                field=self.ficls[0].field,
            ),
            level=self.log_level,
        )
        global logger
        logger = logging.getLogger("CONFIG")

    def cleanup_directories(self):
        logger.info("Removing failed directories.")

        # Iterate over each FICL
        for ficl in self.ficls:
            # Iterate over each object in FICL
            for object in tqdm(ficl.objects, unit="dir", leave=False):
                # Iterate over each expected product file
                for required_file_name in REQUIRED_PRODUCT_FILES:
                    # Remove product directory for FICLO if any product missing
                    product_path = get_path(
                        name=required_file_name,
                        path_settings=self.roots,
                        ficl=ficl,
                        object=object,
                    )
                    if not product_path.exists():
                        product_ficlo_path = get_path(
                            name="product_ficlo",
                            path_settings=self.roots,
                            ficl=ficl,
                            object=object,
                        )
                        shutil.rmtree(product_ficlo_path, ignore_errors=True)

                # Iterate over each expected output file
                if isinstance(self.morphology, GALFITSettings):
                    required_output_files = REQUIRED_GALFIT_OUTPUT_FILES
                elif isinstance(self.morphology, ImcascadeSettings):
                    required_output_files = REQUIRED_IMCASCADE_OUTPUT_FILES
                else:
                    required_output_files = REQUIRED_PYSERSIC_OUTPUT_FILES
                for required_file_name in required_output_files:
                    # Remove output directory for FICLO if any product missing
                    product_path = get_path(
                        name=required_file_name,
                        path_settings=self.roots,
                        ficl=ficl,
                        object=object,
                    )
                    if not product_path.exists():
                        product_ficlo_path = get_path(
                            name="output_ficlo",
                            path_settings=self.roots,
                            ficl=ficl,
                            object=object,
                        )
                        shutil.rmtree(product_ficlo_path, ignore_errors=True)

    def write(self):
        logger.info("Recording runtime settings.")

        # Initialize empty dict for writing
        settings = {}

        # Add paths as strings
        settings["roots"] = {}
        for root_name in self.roots.__dict__:
            settings["roots"][root_name] = str(self.roots.__dict__[root_name])

        # Add run details
        settings["date_time"] = self.date_time
        settings["run_number"] = self.run_number
        settings["progress_bar"] = self.progress_bar
        settings["log_level"] = self.log_level
        settings["process_count"] = self.process_count
        settings["process_id"] = self.process_id

        # Add morphology wrapper as a str
        if self.morphology is not None:
            if isinstance(self.morphology, GALFITSettings):
                settings["morphology"] = "GALFIT"
            elif isinstance(self.morphology, ImcascadeSettings):
                settings["morphology"] = "imcascade"
            else:
                settings["morphology"] = "pysersic"

        # Add stages as a list of stages ran
        if self.stages is not None:
            settings["stages"] = []
            for stage in self.stages.__dict__:
                if self.stages.__dict__[stage]:
                    settings["stages"].append(stage)

        # Add remake flags as a list of products remade
        if self.remake is not None:
            settings["remake"] = []
            for product in self.remake.__dict__:
                if self.remake.__dict__[product]:
                    settings["remake"].append(product)

        # Add FICLs as a list of dicts
        settings["ficls"] = []
        for ficl in self.ficls:
            settings["ficls"].append(ficl.__dict__)
            settings["ficls"][-1]["pixscale"] = {
                "x": ficl.pixscale[0],
                "y": ficl.pixscale[1],
            }

        # Write settings to file
        settings_path = get_path(
            name="run_settings", runtime_settings=self, field=self.ficls[0].field
        )
        yaml.dump(settings, open(settings_path, mode="w"), sort_keys=False)


class ScienceSettings(BaseModel):
    pass


# Functions


## Tertiary


def get_priority_path(
    name: str, cli_settings: dict, file_settings: dict
) -> Path | None:
    # Try getting preferred setting and casting to path object
    try:
        path_str = get_priority_setting(name, cli_settings, file_settings)
        return misc.get_path_obj(path_str)

    # If setting unset or invalid, return None
    except:
        return


def get_priority_stage(
    stage: str, cli_settings: dict, file_settings: dict
) -> bool | None:
    # Return opposite of flag from CLI call if set
    if cli_settings[f"skip_{stage}"] is not None:
        return not cli_settings[f"skip_{stage}"]

    # Return flag from YAML file if set
    elif ("stages" in file_settings) and (stage in file_settings["stages"]):
        return True

    # Return None if unset


def get_priority_remake(
    product: str, cli_settings: dict, file_settings: dict
) -> bool | None:
    # Return opposite of flag from CLI call if set
    if cli_settings[f"remake_{product}"] is not None:
        return not cli_settings[f"remake_{product}"]

    # Return flag from YAML file if set
    elif ("remake" in file_settings) and (product in file_settings["remake"]):
        return True

    # Return None if unset


def validate_batch_settings(
    process_count: int | None,
    process_id: int | None,
    first_object: int | None,
    last_object: int | None,
):
    # Terminate if invalid number of processes
    if process_count is not None:
        assert process_count > 0, f"Invalid # processes {process_count}."

    # Terminate if invalid process ID
    if process_id is not None:
        assert process_id >= 0, f"Invalid process ID {process_id}."
        if process_count is not None:
            assert (
                process_id < process_count
            ), f"Invalid process ID {process_id} for # processes {process_count}."

    # Terminate if invalid first object
    if first_object is not None:
        assert first_object > 0, f"Invalid first object ID {first_object}."

    # Terminate if invalid last object
    if last_object is not None:
        assert last_object > 0, f"Invalid last object ID {last_object}."

    # Terminate if invalid object range
    if (first_object is not None) and (last_object is not None):
        assert (
            first_object <= last_object
        ), f"Invalid object range {first_object} to {last_object}."


def missing_input(
    input_root: Path,
    field: str,
    image_version: str,
    catalog_version: str,
    filter: str | None = None,
) -> bool:
    # Get list of path names of required input files
    required_inputs = REQUIRED_FIC_INPUTS if filter is None else REQUIRED_FICL_INPUTS

    # Iterate over each required input file
    for required_input in required_inputs:
        # Get path to required input file
        input_path = get_path(
            name=required_input,
            input_root=input_root,
            field=field,
            image_version=image_version,
            catalog_version=catalog_version,
            filter=filter,
        )

        # Return true if FIC/FICL missing any input file
        if not input_path.exists():
            return True

    # Return false if FIC/FICL has all input files
    return False


def get_objects(
    input_root: Path,
    field: str,
    image_version: str,
    catalog_version: str,
    objects: list[int] | None,
    process_count: int | None,
    process_id: int | None,
    first_object: int | None,
    last_object: int | None,
) -> list[int]:
    # Base list of objects is all possible object IDs, if in batch mode
    if (
        (objects is None)
        or (process_count is not None)
        or (first_object is not None)
        or (last_object is not None)
    ):
        input_catalog_path = get_path(
            name="input_catalog",
            input_root=input_root,
            field=field,
            image_version=image_version,
            catalog_version=catalog_version,
        )
        new_object_range = science.get_all_objects(input_catalog_path)
        first_possible_object = new_object_range[0]
        last_possible_object = new_object_range[-1]

    # Base list of objects is the sorted list of object IDs, otherwise
    else:
        new_object_range = sorted(objects)

    # Remove all object IDs before first object setting
    if first_object is not None:
        while new_object_range[0] < first_object:
            new_object_range.pop(0)

    # Remove all object IDs after last object setting
    if last_object is not None:
        while new_object_range[-1] > last_object:
            new_object_range.pop(-1)

    # Remove all object IDs not in batch process
    if process_count is not None:
        ## Get sub-range indices and values from batch settings
        start_index, stop_index = misc.get_unique_batch_limits(
            process_id=process_id,
            n_process=process_count,
            n_items=len(new_object_range),
        )
        new_object_range = new_object_range[start_index:stop_index]

        ## Remove objects out of range
        while (len(new_object_range) > 0) and (
            new_object_range[0] < first_possible_object
        ):
            new_object_range.pop(0)
        while (len(new_object_range) > 0) and (
            new_object_range[-1] > last_possible_object
        ):
            new_object_range.pop(-1)

    # Return final list of objects for this catalog (FIC) and runtime
    return new_object_range


def clean_filter(
    input_root: Path, field: str, image_version: str, catalog_version: str, filter: str
) -> str | None:
    # Get path to input science frame
    science_path = get_path(
        name="science",
        input_root=input_root,
        field=field,
        image_version=image_version,
        filter=filter,
    )

    # Filter is valid if science frame exists
    if science_path.exists():
        return filter

    # Filter is invalid otherwise, try other known filter formats
    all_possible_filters = [
        f"{filter}-clear",
        f"clear-{filter}",
        f"{filter}-clearp",
        f"clearp-{filter}",
        filter.replace("clear", "").replace("-", ""),
        filter.replace("clearp", "").replace("-", ""),
    ]

    # Iterate over each known filter format
    for possible_filter in all_possible_filters:
        # Get path to input science frame
        possible_science_path = get_path(
            name="science",
            input_root=input_root,
            field=field,
            image_version=image_version,
            filter=possible_filter,
        )

        # Return new filter name if matching file found
        if possible_science_path.exists():
            pre_logger.debug(f"Filter {possible_filter}: Changing from {filter}.")
            return possible_filter


## Secondary


def get_priority_setting(
    name: str, cli_settings: dict, file_settings: dict
) -> bool | int | Path | list[str] | list[int] | None:
    # Return setting from CLI call if set
    if cli_settings[name] is not None:
        return cli_settings[name]

    # Return setting from YAML file if set
    elif name in file_settings:
        return file_settings[name]

    # Return None if unset


def get_path_settings(cli_settings: dict, file_settings: dict) -> PathSettings:
    # Set initialized flag from parameter from main command
    initialized = cli_settings["initialized"]

    # Get either path objects or None
    settings_pack = [cli_settings, file_settings]
    root = get_priority_path("morphfits_root", *settings_pack)
    input = get_priority_path("input_root", *settings_pack)
    output = get_priority_path("output_root", *settings_pack)
    product = get_priority_path("product_root", *settings_pack)
    run = get_priority_path("run_root", *settings_pack)

    # Input root must be set
    if input is None:
        # Terminate if MorphFITS root is also unset
        if root is None:
            raise ValueError("Terminating - input root unset.")

        # Otherwise assume input root exists and is under root
        else:
            input = root / DEFAULT_INPUT_DIRECTORY_NAME
            assert input.exists(), f"Terminating - input root {input} not found."

    # Input root should exist
    elif not input.exists():
        # Terminate if input root set but does not exist
        if initialized:
            raise FileNotFoundError(f"Terminating - input root {input} not found.")

        # Create input root if main command is initialize
        else:
            input.mkdir(parents=True)

    # Set root directory, if not found, as parent of input root
    if root is None:
        root = input.parent

    # Set product, output, and run directories from root directory
    if output is None:
        output = root / DEFAULT_OUTPUT_DIRECTORY_NAME
    if product is None:
        product = root / DEFAULT_PRODUCT_DIRECTORY_NAME
    if run is None:
        run = root / DEFAULT_RUN_DIRECTORY_NAME

    # Return created and validated object
    return PathSettings(root=root, input=input, output=output, product=product, run=run)


def get_ficls(
    cli_settings: dict,
    file_settings: dict,
    input_root: Path,
    process_count: int | None,
    process_id: int | None,
    first_object: int | None,
    last_object: int | None,
) -> list[FICL]:
    # Get preferred FICLO settings
    settings_pack = [cli_settings, file_settings]
    fields = get_priority_setting("fields", *settings_pack)
    imvers = get_priority_setting("image_versions", *settings_pack)
    catvers = get_priority_setting("catalog_versions", *settings_pack)
    filters = get_priority_setting("filters", *settings_pack)
    objects = get_priority_setting("objects", *settings_pack)

    # Display settings to be searched for from input
    log_str = "Searching for missing settings -"
    if fields is None:
        log_str += " fields,"
    if imvers is None:
        log_str += " image versions,"
    if catvers is None:
        log_str += " catalog versions,"
    if filters is None:
        log_str += " filters,"
    if objects is None:
        log_str += " objects,"
    if log_str[-1] == ",":
        pre_logger.debug(log_str[:-1] + ".")

    # Initialize list of FICLs
    ficls = []

    # Get list of paths to field-level directories (input root subdirectories)
    if fields is None:
        f_paths = misc.get_subdirectories(input_root)

        ## Remove PSFs directory, as it is also an input root subdirectory
        for f_path in f_paths:
            if f_path.name == "psfs":
                f_paths.remove(f_path)
                break
    else:
        f_paths = [input_root / field for field in fields]

    # Iterate over each field directory
    for f_path in f_paths:
        # Get list of paths to image-version-level directories
        if imvers is None:
            fi_paths = misc.get_subdirectories(f_path)
        else:
            fi_paths = [f_path / imver for imver in imvers]

        # Iterate over each image version directory
        for fi_path in fi_paths:
            # Set catalog version to default if unset
            if catvers is None:
                catvers = [DEFAULT_CATALOG_VERSION]

            # Iterate over each catalog version
            for catver in catvers:
                # Skip FIC if missing any input
                if missing_input(
                    input_root=input_root,
                    field=f_path.name,
                    image_version=fi_path.name,
                    catalog_version=catver,
                ):
                    pre_logger.warning(
                        f"FIC {'_'.join([f_path.name,fi_path.name,catver])}: "
                        + "Skipping - missing input files."
                    )
                    continue

                # Get object ID range unique to catalog for FIC
                objects = get_objects(
                    input_root=input_root,
                    field=f_path.name,
                    image_version=fi_path.name,
                    catalog_version=catver,
                    objects=objects,
                    process_count=process_count,
                    process_id=process_id,
                    first_object=first_object,
                    last_object=last_object,
                )

                # Get list of paths to filter-level directories
                if filters is None:
                    ficl_paths = misc.get_subdirectories(fi_path)
                else:
                    ficl_paths = [fi_path / filter for filter in filters]

                # Iterate over each filter directory
                for ficl_path in ficl_paths:
                    # Get cleaned filter name
                    cleaned_filter = clean_filter(
                        input_root=input_root,
                        field=f_path.name,
                        image_version=fi_path.name,
                        catalog_version=catver,
                        filter=ficl_path.name,
                    )

                    # Skip filter if cleaned name not found
                    if cleaned_filter is None:
                        continue

                    # Skip FICL if missing any input
                    if missing_input(
                        input_root=input_root,
                        field=f_path.name,
                        image_version=fi_path.name,
                        catalog_version=catver,
                        filter=cleaned_filter,
                    ):
                        pre_logger.warning(
                            f"FICL {'_'.join([f_path.name,fi_path.name,catver,cleaned_filter])}: "
                            + "Skipping - missing input files."
                        )
                        continue

                    # Get pixscale from science frame
                    try:
                        science_path = get_path(
                            name="science",
                            input_root=input_root,
                            field=f_path.name,
                            image_version=fi_path.name,
                            catalog_version=catver,
                            filter=cleaned_filter,
                        )
                        pixscale = science.get_pixscale(science_path)
                    except Exception as e:
                        logger.error(e)
                        pixscale = [0.04, 0.04]

                    # Create FICL object and add to list
                    ficl = FICL(
                        field=f_path.name,
                        image_version=fi_path.name,
                        catalog_version=catver,
                        filter=cleaned_filter,
                        objects=objects,
                        pixscale=pixscale,
                    )
                    pre_logger.info(f"FICL {ficl}: Adding.")
                    ficls.append(ficl)

    # Return list of FICLs
    return ficls


def get_ficls_to_initialize(cli_settings: dict, file_settings: dict) -> list[FICL]:
    # Get preferred FIL settings
    settings_pack = [cli_settings, file_settings]
    fields = get_priority_setting("fields", *settings_pack)
    imvers = get_priority_setting("image_versions", *settings_pack)
    filters = get_priority_setting("filters", *settings_pack)

    # NOTE Terminates if any of FIL unset, in future can implement discovery
    if (fields is None) or (imvers is None) or (filters is None):
        raise KeyError("FICLs not set for initialization.")

    # Iterate over each FIL permutation
    # NOTE Uses default catalog version
    ficls = []
    for field in fields:
        for imver in imvers:
            for filter in filters:
                # Create FICL object and add to list
                ficl = FICL(
                    field=field,
                    image_version=imver,
                    catalog_version=DEFAULT_CATALOG_VERSION,
                    filter=filter,
                    objects=[-1],
                    pixscale=[-1, -1],
                )
                pre_logger.info(f"FICL {ficl}: Adding.")
                ficls.append(ficl)

    # Return list of FICLs
    return ficls


def get_run_number(path_settings: PathSettings, field: str, date_time: datetime) -> int:
    # Set run number to 1 by default
    run_number = 1

    # If any other processes in same batch use same date time, increase the run
    # number until there is a free directory
    while get_path(
        "run",
        run_root=path_settings.run,
        field=field,
        date_time=date_time,
        run_number=run_number,
    ).exists():
        run_number += 1

    # Return run number
    return run_number


def get_stage_settings(cli_settings: dict, file_settings: dict) -> StageSettings | None:
    # Return None if main command is initialize
    if not cli_settings["initialized"]:
        return

    # Get skip stage flags from CLI call or YAML file
    settings_pack = [cli_settings, file_settings]
    unzip = get_priority_stage("unzip", *settings_pack)
    product = get_priority_stage("product", *settings_pack)
    morphology = get_priority_stage("morphology", *settings_pack)
    catalog = get_priority_stage("catalog", *settings_pack)
    histogram = get_priority_stage("histogram", *settings_pack)
    plot = get_priority_stage("plot", *settings_pack)
    cleanup = get_priority_stage("cleanup", *settings_pack)

    # Create object dict from settings that have been set
    # Set attributes which may be None at this point, if they are set
    # Otherwise they will be set to default as per the class definition
    stage_dict = {}
    if unzip is not None:
        stage_dict["unzip"] = unzip
    if product is not None:
        stage_dict["product"] = product
    if morphology is not None:
        stage_dict["morphology"] = morphology
    if catalog is not None:
        stage_dict["catalog"] = catalog
    if histogram is not None:
        stage_dict["histogram"] = histogram
    if plot is not None:
        stage_dict["plot"] = plot
    if cleanup is not None:
        stage_dict["cleanup"] = cleanup

    # Create and return class instance from settings
    return StageSettings(**stage_dict)


def get_product_settings(
    cli_settings: dict, file_settings: dict
) -> ProductSettings | None:
    # Return None if main command is initialize
    if not cli_settings["initialized"]:
        return

    # Get remake product flags from CLI call or YAML file
    settings_pack = [cli_settings, file_settings]
    remake_all = get_priority_remake("all", *settings_pack)
    stamps = get_priority_remake("stamps", *settings_pack)
    sigmas = get_priority_remake("sigmas", *settings_pack)
    psfs = get_priority_remake("psfs", *settings_pack)
    masks = get_priority_remake("masks", *settings_pack)
    others = get_priority_remake("others", *settings_pack)

    # Create object dict from settings that have been set
    # Set attributes which may be None at this point, if they are set
    # Otherwise they will be set to default as per the class definition
    remake_dict = {}
    if (remake_all is not None) and (remake_all):
        remake_dict["stamps"] = True
        remake_dict["sigmas"] = True
        remake_dict["psfs"] = True
        remake_dict["masks"] = True
        remake_dict["others"] = True
    if stamps is not None:
        remake_dict["stamps"] = stamps
    if sigmas is not None:
        remake_dict["sigmas"] = sigmas
    if psfs is not None:
        remake_dict["psfs"] = psfs
    if masks is not None:
        remake_dict["masks"] = masks
    if others is not None:
        remake_dict["others"] = others

    # Create and return class instance from settings
    return ProductSettings(**remake_dict)


def get_morphology_settings(
    cli_settings: dict, file_settings: dict
) -> GALFITSettings | ImcascadeSettings | PysersicSettings | None:
    # Return None if main command is initialize
    if not cli_settings["initialized"]:
        return

    # Return matching morphology settings instance
    match cli_settings["morphology"]:
        case "galfit":
            # Get GALFIT binary path setting from CLI or YAML
            galfit_path = get_priority_path("galfit_path", cli_settings, file_settings)

            # Terminate if not found, return setting object otherwise
            if galfit_path is None:
                raise KeyError("Terminating - GALFIT binary not provided.")
            elif not galfit_path.exists():
                raise FileNotFoundError("Terminating - GALFIT binary not found.")
            else:
                return GALFITSettings(binary=galfit_path)

        case "imcascade":
            raise NotImplementedError("Terminating - not yet implemented.")
        case "pysersic":
            raise NotImplementedError("Terminating - not yet implemented.")
        case _:
            raise KeyError("Terminating - unknown morphology fitter.")


## Primary


def get_path(
    name: str,
    runtime_settings: RuntimeSettings | None = None,
    path_settings: PathSettings | None = None,
    ficl: FICL | None = None,
    morphfits_root: Path | None = None,
    input_root: Path | None = None,
    output_root: Path | None = None,
    product_root: Path | None = None,
    run_root: Path | None = None,
    field: str | None = None,
    image_version: str | None = None,
    catalog_version: str | None = None,
    filter: str | None = None,
    object: int | None = None,
    date_time: datetime | None = None,
    run_number: int | None = None,
) -> Path:
    """Get the path to a MorphFITS file or directory.

    Parameters
    ----------
    name : str
        Name of path to get.
    runtime_settings : RuntimeSettings | None, optional
        Settings for the runtime of MorphFITS, by default None.
    path_settings : PathSettings | None, optional
        Paths to the root directories of MorphFITS, by default None.
    ficl : FICL | None, optional
        FICL object for current iteration in run, by default None.
    morphfits_root : Path | None, optional
        Path to root of MorphFITS filesystem, by default None.
    input_root : Path | None, optional
        Path to root input directory, by default None.
    output_root : Path | None, optional
        Path to root output directory, by default None.
    product_root : Path | None, optional
        Path to root products directory, by default None.
    run_root : Path | None, optional
        Path to root runs directory, by default None.
    field : str | None, optional
        Field of observation, by default None.
    image_version : str | None, optional
        Image version of science frame, by default None.
    catalog_version : str | None, optional
        Catalog version of science frame, by default None.
    filter : str | None, optional
        Filter used in observation, by default None.
    object : int | None, optional
        Target galaxy or cluster ID in catalog, by default None.
    date_time : datetime | None, optional
        Datetime at start of program run, by default None.
    run_number : int | None, optional
        Number of run in collection with same datetime, by default None.

    Returns
    -------
    Path
        Path to file or directory.

    Raises
    ------
    FileNotFoundError
        Passed path name unrecognized.

    See Also
    --------
    data/paths.yaml
        Data standards dictionary detailing MorphFITS paths.
    """
    # Raise error if path name unknown
    if name not in FILESYSTEM:
        raise FileNotFoundError(f"Unknown MorphFITS path name {name}.")

    # Resolve parameters
    # Prefer directly passed parameters to those from settings objects
    if ficl is not None:
        if field is None:
            field = ficl.field
        if image_version is None:
            image_version = ficl.image_version
        if catalog_version is None:
            catalog_version = ficl.catalog_version
        if filter is None:
            filter = ficl.filter
    if runtime_settings is not None:
        if morphfits_root is None:
            morphfits_root = runtime_settings.roots.root
        if input_root is None:
            input_root = runtime_settings.roots.input
        if output_root is None:
            output_root = runtime_settings.roots.output
        if product_root is None:
            product_root = runtime_settings.roots.product
        if run_root is None:
            run_root = runtime_settings.roots.run
        if date_time is None:
            date_time = runtime_settings.date_time
        if run_number is None:
            run_number = runtime_settings.run_number
    if path_settings is not None:
        if morphfits_root is None:
            morphfits_root = path_settings.root
        if input_root is None:
            input_root = path_settings.input
        if output_root is None:
            output_root = path_settings.output
        if product_root is None:
            product_root = path_settings.product
        if run_root is None:
            run_root = path_settings.run

    # Get path as str template from dict
    path = FILESYSTEM[name]

    # Input PSFs - STSci names with uppercase filter names
    if name == "input_psf":
        # Get main filter from pairs like 'f140w-clear'
        if "-" in filter:
            filter_1, filter_2 = filter.split("-")
            filter = filter_1 if "clear" in filter_2 else filter_2

        # Replace filter template with uppercase main filter name
        path = path.replace("{L}", filter.upper())

    # Replace template in path str with passed value
    if morphfits_root is not None:
        path = path.replace("[root]", str(morphfits_root))
    if input_root is not None:
        path = path.replace("[i]", input_root.name)
        path = path.replace("[root]", str(input_root.parent))
    if output_root is not None:
        path = path.replace("[o]", output_root.name)
        path = path.replace("[root]", str(output_root.parent))
    if product_root is not None:
        path = path.replace("[p]", product_root.name)
        path = path.replace("[root]", str(product_root.parent))
    if run_root is not None:
        path = path.replace("[r]", run_root.name)
        path = path.replace("[root]", str(run_root.parent))
    if field is not None:
        path = path.replace("{F}", field)
    if image_version is not None:
        path = path.replace("{I}", image_version)
    if catalog_version is not None:
        path = path.replace("{C}", catalog_version)
    if filter is not None:
        path = path.replace("{L}", filter)
    if object is not None:
        path = path.replace("{O}", str(object))
    if date_time is not None:
        path = path.replace("{D}", misc.get_str_from_datetime(date_time=date_time))
    if run_number is not None:
        path = path.replace("{N}", misc.get_str_from_run_number(run_number=run_number))

    # Science, exposure, weights images - can contain either 'drc' or 'drz'
    if "{z}" in path:
        # Get paths to both 'drc' and 'drz'
        path_c = misc.get_path_obj(path_like=path.replace("{z}", "c"))
        path_z = misc.get_path_obj(path_like=path.replace("{z}", "z"))

        # Return option that exists
        if path_z.exists():
            return path_z
        else:
            return path_c

    # Return resolved path object
    return misc.get_path_obj(path_like=path)


def get_runtime_settings(cli_settings: dict, file_settings: dict) -> RuntimeSettings:
    # Get initialized flag from parameter passed from main command
    initialized = cli_settings["initialized"]

    # Get settings list to unpack for function calls
    settings_pack = [cli_settings, file_settings]

    # Get root paths
    roots = get_path_settings(*settings_pack)

    # Get primitive type runtime setting attributes
    date_time = datetime.now()
    progress_bar = get_priority_setting("progress_bar", *settings_pack)
    log_level = get_priority_setting("log_level", *settings_pack)
    process_count = get_priority_setting("batch_n_process", *settings_pack)
    process_id = get_priority_setting("batch_process_id", *settings_pack)
    first_object = get_priority_setting("first_object", *settings_pack)
    last_object = get_priority_setting("last_object", *settings_pack)

    # Validate batch mode settings
    validate_batch_settings(process_count, process_id, first_object, last_object)

    # Get list of FICLs
    if initialized:
        ficls = get_ficls(
            cli_settings=cli_settings,
            file_settings=file_settings,
            input_root=roots.input,
            process_count=process_count,
            process_id=process_id,
            first_object=first_object,
            last_object=last_object,
        )
    else:
        ficls = get_ficls_to_initialize(*settings_pack)

    # Get run number from field and date time
    run_number = get_run_number(roots, ficls[0].field, date_time)

    # Get list of stages to run
    stages = get_stage_settings(*settings_pack)

    # Get list of products to remake
    remake = get_product_settings(*settings_pack)

    # Get settings for morphology fitter
    morphology = get_morphology_settings(*settings_pack)

    # Create object dict from settings that have been set
    ## Set attributes which have definitely been set by this point
    runtime_dict = {
        "roots": roots,
        "date_time": date_time,
        "run_number": run_number,
        "ficls": ficls,
    }

    ## Set attributes which may be None at this point, if they are set
    ## Otherwise they will be set to default as per the class definition
    if process_count is not None:
        runtime_dict["process_count"] = process_count
    if process_id is not None:
        runtime_dict["process_id"] = process_id
    if progress_bar is not None:
        runtime_dict["progress_bar"] = progress_bar
    if log_level is not None:
        runtime_dict["log_level"] = log_level
    if stages is not None:
        runtime_dict["stages"] = stages
    if remake is not None:
        runtime_dict["remake"] = remake
    if morphology is not None:
        runtime_dict["morphology"] = morphology

    # Create class instance from settings
    runtime_settings = RuntimeSettings(**runtime_dict)

    # Create directories from path settings and FICLs
    runtime_settings.setup_directories(initialized)

    # Return runtime settings object
    return runtime_settings


def get_science_settings(cli_settings: dict, file_settings: dict) -> ScienceSettings:
    pass


def get_settings(
    config_path: Path | None = None,
    morphfits_root: Path | None = None,
    input_root: Path | None = None,
    output_root: Path | None = None,
    product_root: Path | None = None,
    run_root: Path | None = None,
    batch_n_process: int | None = None,
    batch_process_id: int | None = None,
    fields: list[str] | None = None,
    image_versions: list[str] | None = None,
    catalog_versions: list[str] | None = None,
    filters: list[str] | None = None,
    objects: list[int] | None = None,
    first_object: int | None = None,
    last_object: int | None = None,
    progress_bar: bool | None = None,
    log_level: str | None = None,
    skip_unzip: bool | None = None,
    skip_product: bool | None = None,
    skip_morphology: bool | None = None,
    skip_catalog: bool | None = None,
    skip_histogram: bool | None = None,
    skip_plot: bool | None = None,
    skip_cleanup: bool | None = None,
    remake_all: bool | None = None,
    remake_stamps: bool | None = None,
    remake_sigmas: bool | None = None,
    remake_psfs: bool | None = None,
    remake_masks: bool | None = None,
    remake_others: bool | None = None,
    morphology: str | None = None,
    galfit_path: Path | None = None,
    initialized: bool | None = None,
) -> tuple[RuntimeSettings, ScienceSettings]:
    # Get CLI settings as dict from passed parameters
    cli_settings = locals()

    # Get file settings as dict from YAML file
    if config_path is None:
        file_settings = {}
    else:
        file_settings = yaml.safe_load(open(config_path, mode="r"))

    # Get settings list to unpack for function calls
    settings_pack = [cli_settings, file_settings]

    # Create a temporary logger
    pre_log_file = tempfile.NamedTemporaryFile()
    log_level = get_priority_setting("log_level", *settings_pack)
    base_logger = logs.create_logger(filename=pre_log_file.name, level=log_level)
    global pre_logger
    pre_logger = logging.getLogger("CONFIG")
    pre_logger.info("Loading runtime settings.")

    # Get runtime and science settings from file and CLI settings
    runtime_settings = get_runtime_settings(*settings_pack)
    science_settings = get_science_settings(*settings_pack)

    # Remove pre-program loggers
    base_logger.handlers.clear()
    pre_logger.handlers.clear()
    pre_log_file.close()

    # Create logging objects
    runtime_settings.setup_loggers()

    # Return runtime and science settings
    return runtime_settings, science_settings
